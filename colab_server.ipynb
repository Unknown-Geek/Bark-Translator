{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "unique_id_here"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install flask transformers torch pillow flask-cors accelerate pyngrok\n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from PIL import Image\n",
    "import io\n",
    "from pyngrok import ngrok, conf\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app, resources={r\"/*\": {\n",
    "    \"origins\": [\n",
    "        \"http://127.0.0.1:5000\",  # Frontend\n",
    "        \"http://localhost:5000\",   # Frontend alternate\n",
    "        \"http://127.0.0.1:5001\",  # Backend\n",
    "        \"http://localhost:5001\"    # Backend alternate\n",
    "    ]\n",
    "}})\n",
    "\n",
    "# Configure ngrok\n",
    "NGROK_AUTH_TOKEN = \"2oHUyMyGNJuD34GO6NdGJd8KAxd_3TyYzUGLMA9DvgUopNRw3\"\n",
    "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "\n",
    "# Update port to 5002\n",
    "PORT = 5002\n",
    "\n",
    "try:\n",
    "    print('Loading AI models...')\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # Adjust model parameters for better performance\n",
    "    image_captioner = pipeline(\n",
    "        \"image-to-text\", \n",
    "        model=\"microsoft/git-large-coco\",\n",
    "        device=device,\n",
    "        max_new_tokens=50  # Limit caption length\n",
    "    )\n",
    "    \n",
    "    # Update story model initialization\n",
    "    story_model_name = \"gpt2-large\"\n",
    "    story_tokenizer = AutoTokenizer.from_pretrained(story_model_name, padding_side='left')\n",
    "    story_tokenizer.pad_token = story_tokenizer.eos_token\n",
    "    \n",
    "    story_model = AutoModelForCausalLM.from_pretrained(\n",
    "        story_model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        pad_token_id=story_tokenizer.eos_token_id\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f'Models loaded successfully on {device}!')\n",
    "    \n",
    "    def generate_horror_story(caption):\n",
    "        \"\"\"Generate a longer, structured horror story\"\"\"\n",
    "        prompt = f\"\"\"Scene: {caption}\n",
    "\n",
    "Write a terrifying horror story with the following structure:\n",
    "1. Set the dark, eerie atmosphere\n",
    "2. Introduce the trapped characters\n",
    "3. Build tension through mysterious sounds and events\n",
    "4. Create a climactic confrontation\n",
    "5. End with a chilling revelation\n",
    "\n",
    "Story:\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        inputs = story_tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "        attention_mask = torch.ones(inputs.shape, device=device)\n",
    "        \n",
    "        outputs = story_model.generate(\n",
    "            inputs.to(device),\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=800,  # Longer story\n",
    "            min_length=400,  # Ensure minimum length\n",
    "            do_sample=True,  # Enable sampling\n",
    "            temperature=0.85,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2,\n",
    "            no_repeat_ngram_size=3,\n",
    "            num_beams=5,\n",
    "            early_stopping=True,\n",
    "            pad_token_id=story_tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "        story = story_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return story[len(prompt):].strip()\n",
    "    \n",
    "    @app.after_request\n",
    "    def after_request(response):\n",
    "        response.headers.add('Access-Control-Allow-Origin', '*')\n",
    "        response.headers.add('Access-Control-Allow-Headers', 'Content-Type')\n",
    "        response.headers.add('Access-Control-Allow-Methods', 'GET,POST,OPTIONS')\n",
    "        response.headers.add('Connection', 'close')\n",
    "        return response\n",
    "    \n",
    "    @app.route('/generate_story', methods=['POST', 'OPTIONS'])\n",
    "    def generate_story():\n",
    "        if request.method == 'OPTIONS':\n",
    "            return '', 204\n",
    "        try:\n",
    "            if 'image' not in request.files:\n",
    "                return jsonify({'status': 'error', 'message': 'No image provided'}), 400\n",
    "                \n",
    "            image_file = request.files['image']\n",
    "            image = Image.open(io.BytesIO(image_file.read()))\n",
    "            \n",
    "            caption = image_captioner(image)[0]['generated_text']\n",
    "            story = generate_horror_story(caption)\n",
    "            \n",
    "            return jsonify({\n",
    "                'status': 'success',\n",
    "                'caption': caption,\n",
    "                'story': story\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            return jsonify({'status': 'error', 'message': str(e)}), 500\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error setting up server: {str(e)}\")\n",
    "    raise e\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Start ngrok when the app starts\n",
    "    ngrok.kill()  # Kill any existing tunnels\n",
    "    public_url = ngrok.connect(\n",
    "        addr=PORT,\n",
    "        proto=\"http\"  # Force HTTP protocol\n",
    "    )\n",
    "    print(f'Ngrok tunnel established! Public URL: {public_url}')\n",
    "    # Run app locally\n",
    "    app.run(host='127.0.0.1', port=PORT)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Horror Story Generator API",
   "private_outputs": true
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
